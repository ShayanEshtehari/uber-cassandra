{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql.functions import mean, col, desc, count, isnull, isnan, when, rank, sum\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.master\", \"spark://127.0.0.1:7077\") \\\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(logLevel='ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler, StandardScalerModel\n",
    "\n",
    "from pyspark.ml.clustering import KMeansModel, KMeans\n",
    "seed = 383\n",
    "K = 4\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_file = \"uber-raw-data-aug14.csv\"\n",
    "uber = spark.read.csv(uber_file, header=True, inferSchema=True)\n",
    "\n",
    "train, test = uber.randomSplit([0.8, 0.2], seed=seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.73778073582314,-73.97016031316464]\n",
      "[0.04362806084687051,0.06148272834516592]\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['Lat', 'Lon'], outputCol=\"vector_features\")\n",
    "uber = assembler.transform(uber)\n",
    "\n",
    "scaler = StandardScaler(inputCol='vector_features', outputCol='vector_features_std')\n",
    "scaler_model = scaler.fit(uber)\n",
    "uber = scaler_model.transform(uber)\n",
    "\n",
    "print(scaler_model.mean)\n",
    "print(scaler_model.std)\n",
    "print(scaler_model.getWithMean())\n",
    "print(scaler_model.getWithStd())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uber 20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.73779190118132,-73.97022777754322]\n",
      "[0.04344944241798461,0.06140209835630617]\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['Lat', 'Lon'], outputCol=\"vector_features\")\n",
    "test = assembler.transform(test)\n",
    "\n",
    "scaler = StandardScaler(inputCol='vector_features', outputCol='vector_features_std')\n",
    "scaler_model = scaler.fit(test)\n",
    "test = scaler_model.transform(test)\n",
    "\n",
    "print(scaler_model.mean)\n",
    "print(scaler_model.std)\n",
    "print(scaler_model.getWithMean())\n",
    "print(scaler_model.getWithStd())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(vector_features_std=DenseVector([933.8577, -1203.6616]))]\n",
      "[Row(vector_features_std=DenseVector([937.6967, -1205.2422]))]\n"
     ]
    }
   ],
   "source": [
    "x= uber.select(\"vector_features_std\").filter((col(\"Date/Time\") == '8/1/2014 0:00:00') & (col(\"Lat\") == 40.7424) & (col(\"Lon\") == -74.0044)).collect()\n",
    "print(x)\n",
    "\n",
    "y= test.select(\"vector_features_std\").filter((col(\"Date/Time\") == '8/1/2014 0:00:00') & (col(\"Lat\") == 40.7424) & (col(\"Lon\") == -74.0044)).collect()\n",
    "print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relearn Scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_file = \"uber-raw-data-aug14.csv\"\n",
    "uber = spark.read.csv(uber_file, header=True, inferSchema=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uber 80 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.737777944820685,-73.97014344910451]\n",
      "[0.043672628138981366,0.06150290156661267]\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['Lat', 'Lon'], outputCol=\"vector_features\")\n",
    "train = assembler.transform(train)\n",
    "\n",
    "scaler = StandardScaler(inputCol='vector_features', outputCol='vector_features_std')\n",
    "scaler_model = scaler.fit(train)\n",
    "train = scaler_model.transform(train)\n",
    "\n",
    "print(scaler_model.mean)\n",
    "print(scaler_model.std)\n",
    "print(scaler_model.getWithMean())\n",
    "print(scaler_model.getWithStd())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('8/1/2014 0:00:00', 40.7424, -74.0044, 'B02598', (Vectors.dense([40.7424, -74.0044])))], [\"Date/Time\", \"Lat\", \"Lon\", \"Base\", \"vector_features\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(vector_features_std=DenseVector([932.9047, -1203.2668]))]\n"
     ]
    }
   ],
   "source": [
    "df = scaler_model.transform(df)\n",
    "\n",
    "# df.select(\"vector_features_std\").show(truncate=False)\n",
    "# df.show(truncate=False)\n",
    "\n",
    "a= df.select(\"vector_features_std\").filter((col(\"Date/Time\") == '8/1/2014 0:00:00') & (col(\"Lat\") == 40.7424) & (col(\"Lon\") == -74.0044)).collect()\n",
    "print(a)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add new row to 80 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = train.union(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform without train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(vector_features_std=DenseVector([932.9047, -1203.2668]))]\n"
     ]
    }
   ],
   "source": [
    "new_df = scaler_model.transform(new_df.select(\"Date/Time\", \"Lat\", \"Lon\", \"Base\", \"vector_features\"))\n",
    "\n",
    "b= new_df.select(\"vector_features_std\").filter((col(\"Date/Time\") == '8/1/2014 0:00:00') & (col(\"Lat\") == 40.7424) & (col(\"Lon\") == -74.0044)).collect()\n",
    "print(b)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(vector_features_std=DenseVector([932.9054, -1203.2674]))]\n",
      "[40.73777795178752,-73.97014350073948]\n",
      "[0.043672595593661455,0.06150286959490978]\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol='vector_features', outputCol='vector_features_std')\n",
    "scaler_model = scaler.fit(new_df.select(\"Date/Time\", \"Lat\", \"Lon\", \"Base\", \"vector_features\"))\n",
    "new_df = scaler_model.transform(new_df.select(\"Date/Time\", \"Lat\", \"Lon\", \"Base\", \"vector_features\"))\n",
    "\n",
    "c= new_df.select(\"vector_features_std\").filter((col(\"Date/Time\") == '8/1/2014 0:00:00') & (col(\"Lat\") == 40.7424) & (col(\"Lon\") == -74.0044)).collect()\n",
    "print(c)\n",
    "\n",
    "print(scaler_model.mean)\n",
    "print(scaler_model.std)\n",
    "print(scaler_model.getWithMean())\n",
    "print(scaler_model.getWithStd())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n",
      "[1.4142135623730951]\n",
      "[1.414213562373095]\n",
      "+-----+-------------------+\n",
      "|    a|             output|\n",
      "+-----+-------------------+\n",
      "|[0.0]|              [0.0]|\n",
      "|[2.0]|[1.414213562373095]|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "\n",
    "df = spark.createDataFrame([(Vectors.dense([0.0]),), (DenseVector([2.0]),)], [\"a\"])\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.setInputCol(\"a\")\n",
    "standardScaler.setOutputCol(\"scaled\")\n",
    "\n",
    "model = standardScaler.fit(df)\n",
    "\n",
    "model.getInputCol()\n",
    "model.setOutputCol(\"output\")\n",
    "\n",
    "print(model.mean)\n",
    "print(model.std)\n",
    "x = model.transform(df)\n",
    "print(x.collect()[1].output)\n",
    "x.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardScalerPath = \"_standard-scaler\"\n",
    "standardScaler.save(standardScalerPath)\n",
    "loadedStandardScaler = StandardScaler.load(standardScalerPath)\n",
    "# loadedStandardScaler = StandardScalerModel.load(standardScalerPath) # err\n",
    "\n",
    "loadedStandardScaler.getWithMean() == standardScaler.getWithMean()\n",
    "loadedStandardScaler.getWithStd() == standardScaler.getWithStd()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(a=DenseVector([0.0]), output=DenseVector([0.0]))]\n"
     ]
    }
   ],
   "source": [
    "modelPath = \"_standard-scaler-model\"\n",
    "model.save(modelPath)\n",
    "loadedModel = StandardScalerModel.load(modelPath)\n",
    "# loadedModel = StandardScaler.load(modelPath) # err\n",
    "loadedModel.std == model.std\n",
    "loadedModel.mean == model.mean\n",
    "loadedModel.transform(df).take(1) == model.transform(df).take(1)\n",
    "\n",
    "print(loadedModel.transform(df).take(1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
